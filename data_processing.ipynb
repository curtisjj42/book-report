{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 16:17:58,569 - Found 404 bookshelves.\n",
      "2024-12-03 16:17:58,570 - Processing genre: Science Fiction\n",
      "2024-12-03 16:17:58,709 - Found 25 books in the selected bookshelf.\n",
      "2024-12-03 16:17:59,304 - Downloaded: gutenberg_bookshelf_data\\Science_Fiction\\raw\\The Mysterious Island Jules Verne 1666 downloads.txt\n",
      "2024-12-03 16:17:59,874 - Downloaded: gutenberg_bookshelf_data\\Science_Fiction\\raw\\The Lost World Arthur Conan Doyle 1692 downloads.txt\n",
      "2024-12-03 16:18:00,134 - Downloaded: gutenberg_bookshelf_data\\Science_Fiction\\raw\\The Stolen Bacillus and Other Incidents H. G. Wells 1243 downloads.txt\n",
      "2024-12-03 16:18:00,441 - Downloaded: gutenberg_bookshelf_data\\Science_Fiction\\raw\\Twenty Thousand Leagues under the Sea Jules Verne 5502 downloads.txt\n",
      "2024-12-03 16:18:00,708 - Downloaded: gutenberg_bookshelf_data\\Science_Fiction\\raw\\Voyage au Centre de la Terre (French) Jules Verne 1535 downloads.txt\n",
      "2024-12-03 16:18:00,713 - Metadata saved to gutenberg_bookshelf_data\\Science_Fiction/metadata.csv\n",
      "2024-12-03 16:18:01,572 - Processed: The Lost World Arthur Conan Doyle 1692 downloads.txt\n",
      "2024-12-03 16:18:03,544 - Processed: The Mysterious Island Jules Verne 1666 downloads.txt\n",
      "2024-12-03 16:18:04,284 - Processed: The Stolen Bacillus and Other Incidents H. G. Wells 1243 downloads.txt\n",
      "2024-12-03 16:18:05,407 - Processed: Twenty Thousand Leagues under the Sea Jules Verne 5502 downloads.txt\n",
      "2024-12-03 16:18:06,290 - Processed: Voyage au Centre de la Terre (French) Jules Verne 1535 downloads.txt\n",
      "2024-12-03 16:18:06,296 - Linguistic features saved to gutenberg_bookshelf_data\\Science_Fiction\\cleaned/linguistic_features.csv\n",
      "2024-12-03 16:18:06,298 - Processing genre: Fantasy\n",
      "2024-12-03 16:18:06,421 - Found 25 books in the selected bookshelf.\n",
      "2024-12-03 16:18:06,763 - Downloaded: gutenberg_bookshelf_data\\Fantasy\\raw\\Le Morte d'Arthur_ Volume 2 Sir Thomas Malory 1978 downloads.txt\n",
      "2024-12-03 16:18:07,060 - Downloaded: gutenberg_bookshelf_data\\Fantasy\\raw\\News from Nowhere; Or, An Epoch of Rest William Morris 1032 downloads.txt\n",
      "2024-12-03 16:18:07,346 - Downloaded: gutenberg_bookshelf_data\\Fantasy\\raw\\Robin Hood J. Walker McSpadden 685 downloads.txt\n",
      "2024-12-03 16:18:07,632 - Downloaded: gutenberg_bookshelf_data\\Fantasy\\raw\\Irish Fairy Tales James Stephens 792 downloads.txt\n",
      "2024-12-03 16:18:07,957 - Downloaded: gutenberg_bookshelf_data\\Fantasy\\raw\\The Mabinogion 1338 downloads.txt\n",
      "2024-12-03 16:18:07,960 - Metadata saved to gutenberg_bookshelf_data\\Fantasy/metadata.csv\n",
      "2024-12-03 16:18:08,691 - Processed: Irish Fairy Tales James Stephens 792 downloads.txt\n",
      "2024-12-03 16:18:10,417 - Processed: Le Morte d'Arthur_ Volume 2 Sir Thomas Malory 1978 downloads.txt\n",
      "2024-12-03 16:18:11,145 - Processed: News from Nowhere; Or, An Epoch of Rest William Morris 1032 downloads.txt\n",
      "2024-12-03 16:18:11,865 - Processed: Robin Hood J. Walker McSpadden 685 downloads.txt\n",
      "2024-12-03 16:18:12,874 - Processed: The Mabinogion 1338 downloads.txt\n",
      "2024-12-03 16:18:12,876 - Linguistic features saved to gutenberg_bookshelf_data\\Fantasy\\cleaned/linguistic_features.csv\n",
      "2024-12-03 16:18:12,880 - Processing genre: Mystery Fiction\n",
      "2024-12-03 16:18:13,002 - Found 13 books in the selected bookshelf.\n",
      "2024-12-03 16:18:13,399 - Downloaded: gutenberg_bookshelf_data\\Mystery_Fiction\\raw\\The Mystery of Edwin Drood Charles Dickens 1126 downloads.txt\n",
      "2024-12-03 16:18:13,690 - Downloaded: gutenberg_bookshelf_data\\Mystery_Fiction\\raw\\Greenmantle John Buchan 440 downloads.txt\n",
      "2024-12-03 16:18:14,115 - Downloaded: gutenberg_bookshelf_data\\Mystery_Fiction\\raw\\The Woman in White Wilkie Collins 2437 downloads.txt\n",
      "2024-12-03 16:18:14,514 - Downloaded: gutenberg_bookshelf_data\\Mystery_Fiction\\raw\\The Works of Edgar Allan Poe — Volume 1 Edgar Allan Poe 3892 downloads.txt\n",
      "2024-12-03 16:18:14,829 - Downloaded: gutenberg_bookshelf_data\\Mystery_Fiction\\raw\\The Works of Edgar Allan Poe — Volume 2 Edgar Allan Poe 11521 downloads.txt\n",
      "2024-12-03 16:18:14,831 - Metadata saved to gutenberg_bookshelf_data\\Mystery_Fiction/metadata.csv\n",
      "2024-12-03 16:18:15,893 - Processed: Greenmantle John Buchan 440 downloads.txt\n",
      "2024-12-03 16:18:16,996 - Processed: The Mystery of Edwin Drood Charles Dickens 1126 downloads.txt\n",
      "2024-12-03 16:18:19,496 - Processed: The Woman in White Wilkie Collins 2437 downloads.txt\n",
      "2024-12-03 16:18:20,497 - Processed: The Works of Edgar Allan Poe — Volume 1 Edgar Allan Poe 3892 downloads.txt\n",
      "2024-12-03 16:18:21,441 - Processed: The Works of Edgar Allan Poe — Volume 2 Edgar Allan Poe 11521 downloads.txt\n",
      "2024-12-03 16:18:21,462 - Linguistic features saved to gutenberg_bookshelf_data\\Mystery_Fiction\\cleaned/linguistic_features.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "import logging\n",
    "\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "BASE_URL = \"https://www.gutenberg.org\"\n",
    "\n",
    "def get_bookshelves():\n",
    "    \"\"\"\n",
    "    Fetches all bookshelves (genres) from Project Gutenberg.\n",
    "    \"\"\"\n",
    "    response = requests.get(f\"{BASE_URL}/ebooks/bookshelf/\")\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Failed to fetch the bookshelves page.\")\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    bookshelves = {}\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        if \"/ebooks/bookshelf/\" in link['href']:\n",
    "            genre_name = link.text.strip()\n",
    "            genre_url = BASE_URL + link['href']\n",
    "            bookshelves[genre_name] = genre_url\n",
    "\n",
    "    logger.info(f\"Found {len(bookshelves)} bookshelves.\")\n",
    "    return bookshelves\n",
    "\n",
    "def fetch_books_from_shelf(shelf_url, output_folder, num_books=10):\n",
    "    \"\"\"\n",
    "    Fetches books from a specific bookshelf URL, removes non-English books, \n",
    "    and cleans the title to exclude download numbers or extra details.\n",
    "    \"\"\"\n",
    "    response = requests.get(shelf_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch books from bookshelf: {shelf_url}\")\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    book_links = []\n",
    "    seen_urls = set()\n",
    "\n",
    "    # Extract book links\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        if link['href'].startswith(\"/ebooks/\") and link['href'].split(\"/\")[-1].isdigit():\n",
    "            full_url = f\"{BASE_URL}{link['href']}.txt.utf-8\"\n",
    "            if full_url not in seen_urls:\n",
    "                seen_urls.add(full_url)\n",
    "                raw_title = link.text.strip()\n",
    "                \n",
    "                # Remove downloads information and filter non-English books\n",
    "                if \"(French)\" in raw_title or \"(German)\" in raw_title or \"(Spanish)\" in raw_title:\n",
    "                    logger.info(f\"Skipping non-English book: {raw_title}\")\n",
    "                    continue\n",
    "\n",
    "                # Clean title to remove extra details like downloads\n",
    "                sanitized_title = re.sub(r\"\\s*\\d+\\s*downloads$\", \"\", raw_title).strip()\n",
    "                sanitized_title = re.sub(r'[\\\\/*?:\"<>|]', \"_\", sanitized_title)\n",
    "                sanitized_title = re.sub(r'\\s+', ' ', sanitized_title).strip()  # Remove extra spaces\n",
    "                \n",
    "                book_links.append({\n",
    "                    \"title\": sanitized_title,\n",
    "                    \"url\": full_url\n",
    "                })\n",
    "\n",
    "    logger.info(f\"Found {len(book_links)} books in the selected bookshelf.\")\n",
    "    if len(book_links) == 0:\n",
    "        logger.warning(\"No books found in this bookshelf.\")\n",
    "        return []\n",
    "\n",
    "    # Select a limited number of books\n",
    "    selected_books = random.sample(book_links, min(num_books, len(book_links)))\n",
    "\n",
    "    # Prepare output folder\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    raw_folder = os.path.join(output_folder, \"raw\")\n",
    "    os.makedirs(raw_folder, exist_ok=True)\n",
    "\n",
    "    metadata = []\n",
    "    for book in selected_books:\n",
    "        title = book[\"title\"]\n",
    "        raw_file_path = os.path.join(raw_folder, f\"{title}.txt\")\n",
    "        if download_gutenberg_text(book[\"url\"], raw_file_path):\n",
    "            metadata.append({\"title\": book[\"title\"], \"url\": book[\"url\"], \"genre\": os.path.basename(shelf_url)})\n",
    "\n",
    "    # Save metadata\n",
    "    if metadata:\n",
    "        metadata_df = pd.DataFrame(metadata)\n",
    "        metadata_df.to_csv(os.path.join(output_folder, \"metadata.csv\"), index=False)\n",
    "        logger.info(f\"Metadata saved to {output_folder}/metadata.csv\")\n",
    "    return metadata\n",
    "\n",
    "\n",
    "\n",
    "def download_gutenberg_text(url, save_path):\n",
    "    \"\"\"\n",
    "    Downloads text from a Project Gutenberg URL and saves it locally.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(save_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(response.text)\n",
    "        logger.info(f\"Downloaded: {save_path}\")\n",
    "        return True\n",
    "    else:\n",
    "        logger.error(f\"Failed to download: {url}\")\n",
    "        return False\n",
    "\n",
    "def preprocess_books(raw_folder, clean_folder):\n",
    "    \"\"\"\n",
    "    Preprocesses all raw text files: cleans the text and extracts linguistic features.\n",
    "    \"\"\"\n",
    "    os.makedirs(clean_folder, exist_ok=True)\n",
    "    features = []\n",
    "\n",
    "    for file_name in os.listdir(raw_folder):\n",
    "        if not file_name.endswith(\".txt\"):\n",
    "            continue\n",
    "\n",
    "        raw_file_path = os.path.join(raw_folder, file_name)\n",
    "        with open(raw_file_path, 'r', encoding='utf-8') as raw_file:\n",
    "            raw_text = raw_file.read()\n",
    "\n",
    "        # Clean text\n",
    "        cleaned_text = clean_text(raw_text)\n",
    "        clean_file_path = os.path.join(clean_folder, file_name)\n",
    "        with open(clean_file_path, 'w', encoding='utf-8') as clean_file:\n",
    "            clean_file.write(cleaned_text)\n",
    "        \n",
    "        # Extract linguistic features\n",
    "        tokens = word_tokenize(cleaned_text)\n",
    "        sentences = sent_tokenize(cleaned_text)\n",
    "        word_freq = Counter(tokens)\n",
    "        vocab_richness = len(set(tokens)) / len(tokens) if tokens else 0\n",
    "        avg_sentence_length = sum(len(word_tokenize(s)) for s in sentences) / len(sentences) if sentences else 0\n",
    "\n",
    "        # Store features\n",
    "        features.append({\n",
    "            \"file_name\": file_name,\n",
    "            \"vocab_richness\": vocab_richness,\n",
    "            \"avg_sentence_length\": avg_sentence_length,\n",
    "            \"num_sentences\": len(sentences),\n",
    "            \"num_words\": len(tokens),\n",
    "            \"most_common_word\": word_freq.most_common(1)[0][0] if word_freq else None\n",
    "        })\n",
    "        logger.info(f\"Processed: {file_name}\")\n",
    "\n",
    "    # Save features to CSV\n",
    "    if features:\n",
    "        features_df = pd.DataFrame(features)\n",
    "        features_df.to_csv(os.path.join(clean_folder, \"linguistic_features.csv\"), index=False)\n",
    "        logger.info(f\"Linguistic features saved to {clean_folder}/linguistic_features.csv\")\n",
    "    else:\n",
    "        logger.warning(\"No features extracted; the dataset might be empty.\")\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the raw text by removing headers, footers, and extra spaces.\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove extra whitespace\n",
    "    start_index = text.find(\"*** START OF THIS PROJECT GUTENBERG EBOOK\")\n",
    "    end_index = text.find(\"*** END OF THIS PROJECT GUTENBERG EBOOK\")\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        text = text[start_index + len(\"*** START OF THIS PROJECT GUTENBERG EBOOK\"):end_index]\n",
    "    return text.strip()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get bookshelves\n",
    "    bookshelves = get_bookshelves()\n",
    "\n",
    "    # Choose genres to process\n",
    "    selected_genres = [\"Science Fiction\", \"Fantasy\", \"Mystery Fiction\"]\n",
    "    output_folder = \"gutenberg_bookshelf_data\"\n",
    "\n",
    "    for genre in selected_genres:\n",
    "        if genre in bookshelves:\n",
    "            logger.info(f\"Processing genre: {genre}\")\n",
    "            shelf_url = bookshelves[genre]\n",
    "            genre_folder = os.path.join(output_folder, genre.replace(\" \", \"_\"))\n",
    "            metadata = fetch_books_from_shelf(shelf_url, genre_folder, num_books=5)\n",
    "            if metadata:\n",
    "                preprocess_books(os.path.join(genre_folder, \"raw\"), os.path.join(genre_folder, \"cleaned\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
