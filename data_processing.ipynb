{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 16:21:45,975 - Found 404 bookshelves.\n",
      "2024-12-03 16:21:45,976 - Processing genre: Science Fiction\n",
      "2024-12-03 16:21:46,123 - Skipping non-English book: Voyage au Centre de la Terre (French)\n",
      "Jules Verne\n",
      "1535 downloads\n",
      "2024-12-03 16:21:46,124 - Skipping non-English book: L'île mystérieuse (French)\n",
      "Jules Verne\n",
      "1436 downloads\n",
      "2024-12-03 16:21:46,125 - Found 23 books in the selected bookshelf.\n",
      "2024-12-03 16:21:46,309 - Downloaded: gutenberg_bookshelf_data\\Science_Fiction\\raw\\The Eyes Have It Philip K. Dick.txt\n",
      "2024-12-03 16:21:46,561 - Downloaded: gutenberg_bookshelf_data\\Science_Fiction\\raw\\The War of the Worlds H. G. Wells.txt\n",
      "2024-12-03 16:21:46,763 - Downloaded: gutenberg_bookshelf_data\\Science_Fiction\\raw\\The Big Trip Up Yonder Kurt Vonnegut.txt\n",
      "2024-12-03 16:21:47,096 - Downloaded: gutenberg_bookshelf_data\\Science_Fiction\\raw\\The Country of the Blind, and Other Stories H. G. Wells.txt\n",
      "2024-12-03 16:21:47,298 - Downloaded: gutenberg_bookshelf_data\\Science_Fiction\\raw\\The Marching Morons C. M. Kornbluth.txt\n",
      "2024-12-03 16:21:47,300 - Metadata saved to gutenberg_bookshelf_data\\Science_Fiction/metadata.csv\n",
      "2024-12-03 16:21:47,402 - Processed: The Big Trip Up Yonder Kurt Vonnegut.txt\n",
      "2024-12-03 16:21:49,208 - Processed: The Country of the Blind, and Other Stories H. G. Wells.txt\n",
      "2024-12-03 16:21:49,272 - Processed: The Eyes Have It Philip K. Dick.txt\n",
      "2024-12-03 16:21:49,453 - Processed: The Marching Morons C. M. Kornbluth.txt\n",
      "2024-12-03 16:21:50,086 - Processed: The War of the Worlds H. G. Wells.txt\n",
      "2024-12-03 16:21:50,088 - Linguistic features saved to gutenberg_bookshelf_data\\Science_Fiction\\cleaned/linguistic_features.csv\n",
      "2024-12-03 16:21:50,090 - Processing genre: Fantasy\n",
      "2024-12-03 16:21:50,247 - Found 25 books in the selected bookshelf.\n",
      "2024-12-03 16:21:50,503 - Downloaded: gutenberg_bookshelf_data\\Fantasy\\raw\\The Sword of Welleran and Other Stories Lord Dunsany.txt\n",
      "2024-12-03 16:21:50,783 - Downloaded: gutenberg_bookshelf_data\\Fantasy\\raw\\Dorothy and the Wizard in Oz L. Frank Baum.txt\n",
      "2024-12-03 16:21:51,081 - Downloaded: gutenberg_bookshelf_data\\Fantasy\\raw\\News from Nowhere; Or, An Epoch of Rest William Morris.txt\n",
      "2024-12-03 16:21:51,360 - Downloaded: gutenberg_bookshelf_data\\Fantasy\\raw\\The Wonderful Wizard of Oz L. Frank Baum.txt\n",
      "2024-12-03 16:21:51,642 - Downloaded: gutenberg_bookshelf_data\\Fantasy\\raw\\Men of Iron Howard Pyle.txt\n",
      "2024-12-03 16:21:51,645 - Metadata saved to gutenberg_bookshelf_data\\Fantasy/metadata.csv\n",
      "2024-12-03 16:21:52,166 - Processed: Dorothy and the Wizard in Oz L. Frank Baum.txt\n",
      "2024-12-03 16:21:52,873 - Processed: Men of Iron Howard Pyle.txt\n",
      "2024-12-03 16:21:53,592 - Processed: News from Nowhere; Or, An Epoch of Rest William Morris.txt\n",
      "2024-12-03 16:21:53,917 - Processed: The Sword of Welleran and Other Stories Lord Dunsany.txt\n",
      "2024-12-03 16:21:54,332 - Processed: The Wonderful Wizard of Oz L. Frank Baum.txt\n",
      "2024-12-03 16:21:54,336 - Linguistic features saved to gutenberg_bookshelf_data\\Fantasy\\cleaned/linguistic_features.csv\n",
      "2024-12-03 16:21:54,337 - Processing genre: Mystery Fiction\n",
      "2024-12-03 16:21:54,455 - Found 13 books in the selected bookshelf.\n",
      "2024-12-03 16:21:54,713 - Downloaded: gutenberg_bookshelf_data\\Mystery_Fiction\\raw\\The Thirty-Nine Steps John Buchan.txt\n",
      "2024-12-03 16:21:55,133 - Downloaded: gutenberg_bookshelf_data\\Mystery_Fiction\\raw\\The Woman in White Wilkie Collins.txt\n",
      "2024-12-03 16:21:55,443 - Downloaded: gutenberg_bookshelf_data\\Mystery_Fiction\\raw\\The Works of Edgar Allan Poe — Volume 3 Edgar Allan Poe.txt\n",
      "2024-12-03 16:21:55,727 - Downloaded: gutenberg_bookshelf_data\\Mystery_Fiction\\raw\\Greenmantle John Buchan.txt\n",
      "2024-12-03 16:21:56,035 - Downloaded: gutenberg_bookshelf_data\\Mystery_Fiction\\raw\\The Moonstone Wilkie Collins.txt\n",
      "2024-12-03 16:21:56,038 - Metadata saved to gutenberg_bookshelf_data\\Mystery_Fiction/metadata.csv\n",
      "2024-12-03 16:21:57,091 - Processed: Greenmantle John Buchan.txt\n",
      "2024-12-03 16:21:59,239 - Processed: The Moonstone Wilkie Collins.txt\n",
      "2024-12-03 16:21:59,692 - Processed: The Thirty-Nine Steps John Buchan.txt\n",
      "2024-12-03 16:22:02,118 - Processed: The Woman in White Wilkie Collins.txt\n",
      "2024-12-03 16:22:03,072 - Processed: The Works of Edgar Allan Poe — Volume 3 Edgar Allan Poe.txt\n",
      "2024-12-03 16:22:03,075 - Linguistic features saved to gutenberg_bookshelf_data\\Mystery_Fiction\\cleaned/linguistic_features.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "import logging\n",
    "\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "BASE_URL = \"https://www.gutenberg.org\"\n",
    "\n",
    "def get_bookshelves():\n",
    "    \"\"\"\n",
    "    Fetches all bookshelves (genres) from Project Gutenberg.\n",
    "    \"\"\"\n",
    "    response = requests.get(f\"{BASE_URL}/ebooks/bookshelf/\")\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Failed to fetch the bookshelves page.\")\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    bookshelves = {}\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        if \"/ebooks/bookshelf/\" in link['href']:\n",
    "            genre_name = link.text.strip()\n",
    "            genre_url = BASE_URL + link['href']\n",
    "            bookshelves[genre_name] = genre_url\n",
    "\n",
    "    logger.info(f\"Found {len(bookshelves)} bookshelves.\")\n",
    "    return bookshelves\n",
    "\n",
    "def fetch_books_from_shelf(shelf_url, output_folder, num_books=10):\n",
    "    \"\"\"\n",
    "    Fetches books from a specific bookshelf URL, removes non-English books, \n",
    "    and cleans the title to exclude download numbers or extra details.\n",
    "    \"\"\"\n",
    "    response = requests.get(shelf_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch books from bookshelf: {shelf_url}\")\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    book_links = []\n",
    "    seen_urls = set()\n",
    "\n",
    "    # Extract book links\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        if link['href'].startswith(\"/ebooks/\") and link['href'].split(\"/\")[-1].isdigit():\n",
    "            full_url = f\"{BASE_URL}{link['href']}.txt.utf-8\"\n",
    "            if full_url not in seen_urls:\n",
    "                seen_urls.add(full_url)\n",
    "                raw_title = link.text.strip()\n",
    "                \n",
    "                # Remove downloads information and filter non-English books\n",
    "                if \"(French)\" in raw_title or \"(German)\" in raw_title or \"(Spanish)\" in raw_title:\n",
    "                    logger.info(f\"Skipping non-English book: {raw_title}\")\n",
    "                    continue\n",
    "\n",
    "                # Clean title to remove extra details like downloads\n",
    "                sanitized_title = re.sub(r\"\\s*\\d+\\s*downloads$\", \"\", raw_title).strip()\n",
    "                sanitized_title = re.sub(r'[\\\\/*?:\"<>|]', \"_\", sanitized_title)\n",
    "                sanitized_title = re.sub(r'\\s+', ' ', sanitized_title).strip()  # Remove extra spaces\n",
    "                \n",
    "                book_links.append({\n",
    "                    \"title\": sanitized_title,\n",
    "                    \"url\": full_url\n",
    "                })\n",
    "\n",
    "    logger.info(f\"Found {len(book_links)} books in the selected bookshelf.\")\n",
    "    if len(book_links) == 0:\n",
    "        logger.warning(\"No books found in this bookshelf.\")\n",
    "        return []\n",
    "\n",
    "    # Select a limited number of books\n",
    "    selected_books = random.sample(book_links, min(num_books, len(book_links)))\n",
    "\n",
    "    # Prepare output folder\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    raw_folder = os.path.join(output_folder, \"raw\")\n",
    "    os.makedirs(raw_folder, exist_ok=True)\n",
    "\n",
    "    metadata = []\n",
    "    for book in selected_books:\n",
    "        title = book[\"title\"]\n",
    "        raw_file_path = os.path.join(raw_folder, f\"{title}.txt\")\n",
    "        if download_gutenberg_text(book[\"url\"], raw_file_path):\n",
    "            metadata.append({\"title\": book[\"title\"], \"url\": book[\"url\"], \"genre\": os.path.basename(shelf_url)})\n",
    "\n",
    "    # Save metadata\n",
    "    if metadata:\n",
    "        metadata_df = pd.DataFrame(metadata)\n",
    "        metadata_df.to_csv(os.path.join(output_folder, \"metadata.csv\"), index=False)\n",
    "        logger.info(f\"Metadata saved to {output_folder}/metadata.csv\")\n",
    "    return metadata\n",
    "\n",
    "\n",
    "\n",
    "def download_gutenberg_text(url, save_path):\n",
    "    \"\"\"\n",
    "    Downloads text from a Project Gutenberg URL and saves it locally.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(save_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(response.text)\n",
    "        logger.info(f\"Downloaded: {save_path}\")\n",
    "        return True\n",
    "    else:\n",
    "        logger.error(f\"Failed to download: {url}\")\n",
    "        return False\n",
    "\n",
    "def preprocess_books(raw_folder, clean_folder):\n",
    "    \"\"\"\n",
    "    Preprocesses all raw text files: cleans the text and extracts linguistic features.\n",
    "    \"\"\"\n",
    "    os.makedirs(clean_folder, exist_ok=True)\n",
    "    features = []\n",
    "\n",
    "    for file_name in os.listdir(raw_folder):\n",
    "        if not file_name.endswith(\".txt\"):\n",
    "            continue\n",
    "\n",
    "        raw_file_path = os.path.join(raw_folder, file_name)\n",
    "        with open(raw_file_path, 'r', encoding='utf-8') as raw_file:\n",
    "            raw_text = raw_file.read()\n",
    "\n",
    "        # Clean text\n",
    "        cleaned_text = clean_text(raw_text)\n",
    "        clean_file_path = os.path.join(clean_folder, file_name)\n",
    "        with open(clean_file_path, 'w', encoding='utf-8') as clean_file:\n",
    "            clean_file.write(cleaned_text)\n",
    "        \n",
    "        # Extract linguistic features\n",
    "        tokens = word_tokenize(cleaned_text)\n",
    "        sentences = sent_tokenize(cleaned_text)\n",
    "        word_freq = Counter(tokens)\n",
    "        vocab_richness = len(set(tokens)) / len(tokens) if tokens else 0\n",
    "        avg_sentence_length = sum(len(word_tokenize(s)) for s in sentences) / len(sentences) if sentences else 0\n",
    "\n",
    "        # Store features\n",
    "        features.append({\n",
    "            \"file_name\": file_name,\n",
    "            \"vocab_richness\": vocab_richness,\n",
    "            \"avg_sentence_length\": avg_sentence_length,\n",
    "            \"num_sentences\": len(sentences),\n",
    "            \"num_words\": len(tokens),\n",
    "            \"most_common_word\": word_freq.most_common(1)[0][0] if word_freq else None\n",
    "        })\n",
    "        logger.info(f\"Processed: {file_name}\")\n",
    "\n",
    "    # Save features to CSV\n",
    "    if features:\n",
    "        features_df = pd.DataFrame(features)\n",
    "        features_df.to_csv(os.path.join(clean_folder, \"linguistic_features.csv\"), index=False)\n",
    "        logger.info(f\"Linguistic features saved to {clean_folder}/linguistic_features.csv\")\n",
    "    else:\n",
    "        logger.warning(\"No features extracted; the dataset might be empty.\")\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the raw text by removing headers, footers, and extra spaces.\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove extra whitespace\n",
    "    start_index = text.find(\"*** START OF THIS PROJECT GUTENBERG EBOOK\")\n",
    "    end_index = text.find(\"*** END OF THIS PROJECT GUTENBERG EBOOK\")\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        text = text[start_index + len(\"*** START OF THIS PROJECT GUTENBERG EBOOK\"):end_index]\n",
    "    return text.strip()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get bookshelves\n",
    "    bookshelves = get_bookshelves()\n",
    "\n",
    "    # Choose genres to process\n",
    "    selected_genres = [\"Science Fiction\", \"Fantasy\", \"Mystery Fiction\"]\n",
    "    output_folder = \"gutenberg_bookshelf_data\"\n",
    "\n",
    "    for genre in selected_genres:\n",
    "        if genre in bookshelves:\n",
    "            logger.info(f\"Processing genre: {genre}\")\n",
    "            shelf_url = bookshelves[genre]\n",
    "            genre_folder = os.path.join(output_folder, genre.replace(\" \", \"_\"))\n",
    "            metadata = fetch_books_from_shelf(shelf_url, genre_folder, num_books=5)\n",
    "            if metadata:\n",
    "                preprocess_books(os.path.join(genre_folder, \"raw\"), os.path.join(genre_folder, \"cleaned\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
